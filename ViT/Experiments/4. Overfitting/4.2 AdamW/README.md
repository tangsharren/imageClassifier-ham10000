AdamW 
[parameters](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)

- weight_decay (float, optional, defaults to 0) — The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer.
- adam_beta1 (float, optional, defaults to 0.9) — The beta1 hyperparameter for the AdamW optimizer.
- adam_beta2 (float, optional, defaults to 0.999) — The beta2 hyperparameter for the AdamW optimizer.
- adam_epsilon (float, optional, defaults to 1e-8) — The epsilon hyperparameter for the AdamW optimizer.
